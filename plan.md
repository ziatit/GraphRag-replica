# ğŸ—ï¸ Microservice GraphRAG "Budget Edition" - Implementation Guide

[cite_start]Ten dokument opisuje plan budowy mikroserwisu realizujÄ…cego logikÄ™ GraphRAG opisanÄ… w paperze "From Local to Global"[cite: 2], zoptymalizowanÄ… pod kÄ…tem kosztÃ³w i wydajnoÅ›ci (poziom C0/C1, duÅ¼e chunki).

## 1. Technology Stack

WybÃ³r technologii podyktowany Å‚atwoÅ›ciÄ… konteneryzacji i zgodnoÅ›ciÄ… z bibliotekami uÅ¼ytymi w badaniach.

* **Runtime:** `Python 3.11+` (Wymagany ze wzglÄ™du na typowanie i async).
* **API Framework:** `FastAPI` (Lekki, szybki, natywna obsÅ‚uga asynchronicznoÅ›ci dla rÃ³wnolegÅ‚ych zapytaÅ„ LLM).
* **Graph Engine:** `NetworkX` (WystarczajÄ…cy do trzymania grafu w pamiÄ™ci i serializacji do JSON/Pickle). Nie potrzebujemy Neo4j do MVP.
* [cite_start]**Community Detection:** `graspologic` lub `leidenalg` (Zgodnie z metodologiÄ… paperu: algorytm Leiden [cite: 160, 239]).
* **LLM Integration:** `OpenAI API` (lub `LiteLLM` jako warstwa abstrakcji, jeÅ›li chcesz podpiÄ…Ä‡ lokalnego Ollama/vLLM w przyszÅ‚oÅ›ci).
* **Container:** `Docker` (Multi-stage build dla minimalizacji obrazu).
* **Storage:** `Local File System` (Wolumen Dockera) do trzymania zindeksowanego stanu (`graph.gml`, `community_reports.json`).

---

## 2. Architektura Mikroserwisu

Mikroserwis bÄ™dzie posiadaÅ‚ trzy gÅ‚Ã³wne endpointy odpowiadajÄ…ce fazom z paperu:
1.  `POST /ingest` - Przyjmuje tekst, wykonuje chunking i ekstrakcjÄ™ (buduje graf).
2.  `POST /build-index` - Wykonuje klastrowanie (Leiden) i generuje Community Summaries (C0/C1).
3.  `POST /query` - ObsÅ‚uguje zapytania globalne metodÄ… Map-Reduce.

---

## 3. Lista ZadaÅ„ (Task List)

### Faza 0: Setup Projektu i Kontenera
- [ ] **Inicjalizacja Repo:** Struktura katalogÃ³w (`/app`, `/data`, `Dockerfile`, `requirements.txt`).
- [ ] **Docker Setup:** Przygotowanie `Dockerfile` dla Pythona.
    - *Tip:* Zadbaj o instalacjÄ™ kompilatorÃ³w C++ w obrazie bazowym, sÄ… potrzebne dla `leidenalg`/`igraph`.
- [ ] **Environment:** Konfiguracja `.env` (API Keys, ustawienia modelu).

### Faza 1: Ingest & Graph Extraction (Najkosztowniejsza czÄ™Å›Ä‡)
Celem jest zamiana tekstu na surowy graf. [cite_start]Optymalizacja kosztÃ³w poprzez duÅ¼e chunki[cite: 502].

- [ ] **Chunking Service:** Implementacja podziaÅ‚u tekstu.
    - *Parametry:* Chunk size = 2400 tokenÃ³w, Overlap = 100 tokenÃ³w (optymalizacja kosztÃ³w vs recall).
- [ ] **LLM Client:** Wrapper na API (np. OpenAI) z obsÅ‚ugÄ… retry/backoff (kluczowe przy duÅ¼ej liczbie zapytaÅ„).
- [ ] **Entity & Relation Extraction:** Prompt inÅ¼ynieria.
    - [cite_start]*Zadanie:* Zaimplementuj prompt z **Appendix E.1**.
    - [cite_start]*Optymalizacja:* WyÅ‚Ä…cz "Self-Correction"[cite: 508], aby oszczÄ™dziÄ‡ tokeny.
    - *Output:* Parsowanie odpowiedzi LLM do listy krotek `(Source, Target, Description)`.
- [ ] **Graph Builder:** Budowanie obiektu `NetworkX`.
    - *Logika:* Dodawanie wÄ™zÅ‚Ã³w i krawÄ™dzi.
    - *Entity Resolution:* Prosta normalizacja nazw (lowercase, stripping) - jeÅ›li nazwy identyczne, scalaj opisy.

### Faza 2: Community Detection & Summarization (Tworzenie indeksu)
To tutaj powstaje "Globalna PamiÄ™Ä‡". [cite_start]Skupiamy siÄ™ tylko na wysokich poziomach (Root/C0)[cite: 225].

- [ ] **Hierarchical Clustering:** Implementacja algorytmu Leiden.
    - [cite_start]*Tool:* UÅ¼yj `graspologic.partition.hierarchical_leiden`[cite: 239].
    - *Output:* Mapa `Node ID -> Community ID` (z zachowaniem hierarchii poziomÃ³w).
- [ ] **Context Builder (do Summaries):** Logika przygotowania danych dla LLM.
    - *Algorytm:* Dla kaÅ¼dej spoÅ‚ecznoÅ›ci zbierz jej wÄ™zÅ‚y i krawÄ™dzie. [cite_start]JeÅ›li przekraczajÄ… limit tokenÃ³w, sortuj po `node degree` i ucinaj[cite: 172].
- [ ] **Community Summarization:** Generowanie raportÃ³w.
    - *Zakres:* Generuj **tylko dla poziomu C0 i C1** (Root i High-Level). [cite_start]Ignoruj liÅ›cie (C2/C3) dla oszczÄ™dnoÅ›ci[cite: 290].
    - [cite_start]*Prompt:* Adaptacja promptu z **Appendix E.2**.
- [ ] **Persistence:** Zapisz wygenerowane raporty do pliku JSON (`index_storage`).

### Faza 3: Query Engine (Map-Reduce)
[cite_start]Implementacja logiki "Global Search"[cite: 41, 175].

- [ ] **Map Step (RÃ³wnolegÅ‚a Ocena):**
    - *Input:* Zapytanie uÅ¼ytkownika + Lista wszystkich raportÃ³w z poziomu C0/C1.
    - *Logic:* Asynchroniczne wysÅ‚anie raportÃ³w do LLM (uÅ¼yj `asyncio`).
    - [cite_start]*Prompt:* "OceÅ„ przydatnoÅ›Ä‡ raportu (0-100) i wyciÄ…gnij odpowiedÅº" (Prompt z **Appendix E.3** ).
- [ ] **Filter & Sort:**
    - *Logic:* OdrzuÄ‡ odpowiedzi z `score=0`. Posortuj resztÄ™ malejÄ…co.
- [ ] **Reduce Step (Final Answer):**
    - [cite_start]*Logic:* Sklejaj najlepsze odpowiedzi czÄ…stkowe aÅ¼ do zapeÅ‚nienia Context Window (np. 8k tokenÃ³w)[cite: 183].
    - [cite_start]*Prompt:* Generowanie finalnej odpowiedzi globalnej (Prompt z **Appendix E.4** ).

### Faza 4: API & Integration
- [ ] **FastAPI Endpoints:** SpiÄ™cie logiki w kontrolery.
- [ ] **Background Tasks:** Oznaczenie endpointÃ³w indeksowania jako `BackgroundTasks` w FastAPI (indeksowanie trwa dÅ‚ugo, nie chcemy timeoutu HTTP).
- [ ] **Logging:** Dodanie logÃ³w strukturalnych (ile tokenÃ³w zuÅ¼yto, ile encji wykryto).

---

## 4. PrzykÅ‚adowa struktura plikÃ³w

```text
/graphrag-microservice
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ /app
â”‚   â”œâ”€â”€ main.py              # Entrypoint FastAPI
â”‚   â”œâ”€â”€ config.py            # Ustawienia (chunk size, levels)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ extractor.py     # Logika LLM do wyciÄ…gania encji
â”‚   â”‚   â”œâ”€â”€ graph.py         # ObsÅ‚uga NetworkX i Leiden
â”‚   â”‚   â”œâ”€â”€ search.py        # Logika Map-Reduce
â”‚   â”‚   â””â”€â”€ text_utils.py    # Chunking
â”‚   â”œâ”€â”€ prompts/             # Pliki tekstowe z promptami z AppendiksÃ³w
â”‚   â””â”€â”€ models/              # Pydantic models (Request/Response)
â””â”€â”€ /data                    # Wolumen na zapisany indeks